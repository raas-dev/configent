# See: https://github.com/sigoden/aichat/blob/main/config.example.yaml

# ---- llm ----
model: gemini:gemini-2.5-pro-preview-05-06  # Specify the LLM to use
temperature: 0 # Set default temperature parameter

# ---- behavior ----
save: false # Indicates whether to persist the message (messages.md)
keybindings: vi # Choose keybinding style (emacs, vi)
wrap: auto # Controls text wrapping (no, auto, <max-width>)
wrap_code: false # Enables or disables wrapping of code blocks

# ---- function-calling ----
# Visit https://github.com/sigoden/llm-functions for setup instructions
function_calling: true           # Enables or disables function calling (Globally).
mapping_tools:                   # Alias for a tool or toolset
  fs: 'fs_cat,fs_ls,fs_mkdir,fs_rm,fs_write'
use_tools: null                  # Which tools to use by default. (e.g. 'fs,web_search')

# ---- prelude ----
cmd_prelude: null                # Set a default role or session for CMD mode (e.g. role:<name>, session:<name>, <session>:<role>)
repl_prelude: "role:chat"               # Set a default role or session for REPL mode (e.g. role:<name>, session:<name>, <session>:<role>)
agent_prelude: null              # Set a session to use when starting a agent. (e.g. temp, default)

# ---- session ----
# Controls the persistence of the session. if true, auto save; if false, not save; if null, asking the user
save_session: false
# Compress session when token count reaches or exceeds this threshold
compress_threshold: 16000
# Text prompt used for creating a concise summary of session message
summarize_prompt: 'Summarize the discussion briefly in 200 words or less to use as a prompt for future context.'
# Text prompt used for including the summary of the entire session
summary_prompt: 'This is a summary of the chat history as a recap: '

# ---- RAG ----
# See [RAG-Guide](https://github.com/sigoden/aichat/wiki/RAG-Guide) for more details.
rag_embedding_model: gemini:text-embedding-004
rag_reranker_model: null                    # Specifies the rerank model to use
rag_chunk_size: 1500
rag_chunk_overlap: 75

# Define document loaders to control how RAG and `.file`/`--file` load files of specific formats.
document_loaders:
  # You can add custom loaders using the following syntax:
  #   <file-extension>: <command-to-load-the-file>
  # Note: Use `$1` for input file and `$2` for output file. If `$2` is omitted, use stdout as output.
  #pdf: 'pdftotext $1 -'                         # Load .pdf file, see https://poppler.freedesktop.org to set up pdftotext
  docx: 'pandoc --to plain $1'                  # Load .docx file, see https://pandoc.org to set up pandoc

# ---- appearance ----
highlight: true # Controls syntax highlighting
light_theme: false # Activates a light color theme when true. env: AICHAT_LIGHT_THEME
# Custom REPL left/right prompts, see https://github.com/sigoden/aichat/wiki/Custom-REPL-Prompt for more details
left_prompt:
  '{?agent {color.purple}{agent}}{!agent {color.green}{role}}{?rag {color.yellow}@{rag}}{color.reset}> '
right_prompt:
  '{?consume_tokens {color.blue}{consume_tokens}({consume_percent}%)}{!consume_tokens {color.blue}{consume_tokens}} {?session {color.cyan}({session})}{color.reset}'

# ---- clients ----
clients:
  - type: gemini
  - type: claude
  - type: openai

  # OpenRouter
  - type: openai-compatible
    name: openrouter

  # together.ai
  - type: openai-compatible
    name: together

  # local
  - type: openai-compatible
    name: ollama
    api_base: http://127.0.0.1:11434/v1
    models:
      - name: gemma3:12b
        max_input_tokens: 131072
        supports_function_calling: true
        supports_vision: true
      - name: deepseek-r1:8b
        max_input_tokens: 131072
      - name: nomic-embed-text
        type: embedding
      - name: jina-reranker-v2-base-multilingual
        type: reranker

  # AWS Bedrock
  - type: bedrock
    name: aws   # export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
    region: eu-central-1

  # Azure AI Foundry
  - type: azure-openai
    name: azure_foundry   # export AZURE_FOUNDRY_API_BASE and _API_KEY
    models:
      - name: o4-mini
        max_input_tokens: 200000
        max_output_tokens: 100000
        supports_vision: true
        supports_function_calling: true
        system_prompt_prefix: Formatting re-enabled
        patch:
          body:
            reasoning_effort: high
            max_tokens: null
            temperature: null
            top_p: null
      - name: gpt-4.1-nano
        max_input_tokens: 1047576
        max_output_tokens: 32768
        supports_vision: true
        supports_function_calling: true
